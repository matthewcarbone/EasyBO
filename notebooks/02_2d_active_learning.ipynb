{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa0b25-1b8d-467a-9b04-b81d2bccce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a7574-cdfa-4bac-977a-d15d011763d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b57b9-1372-4443-9f3e-fe612b2b04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "plt.rcParams[\"figure.figsize\"] = (3, 2)\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc155cd-a35f-43ad-ba31-d4d7fdc4e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easybo.gp import EasySingleTaskGPRegressor\n",
    "from easybo.bo import ask\n",
    "from easybo.logger import logging_mode\n",
    "from easybo.utils import get_dummy_2d_data, set_grids, grids_to_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f9c45-2f5c-4a5b-9862-b93cc9dda616",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Active learning in a 2d space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf93f3-7ce2-4da8-91a7-7bfb18a3f43e",
   "metadata": {},
   "source": [
    "While it isn't really an \"interesting\" form of Bayesian Optimization, it is a useful one. Pure active learning is a 100% exploratory algorithm which seeks to minimize the overall variance of the estimator subject to some boundary constraints. For example, given an estimator with mean $\\mu(x)$ and standard deviation $\\sigma(x),$ the Active Learning policy, also called Maximum Variance (MaxVar) is defined by acquisition function\n",
    "\n",
    "$$ A(x) = \\sigma^2(x)$$\n",
    "\n",
    "As the goal is to maximize the acquisition function, new points will always be sampled where the variance of the estimator is highest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ea64f-dd03-4fe8-a3ce-795a8907fc82",
   "metadata": {},
   "source": [
    "## The oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4484d-848c-4222-82e2-85637d9a35c1",
   "metadata": {},
   "source": [
    "Every problem in autonomous experimentation must have a source of truth. This source of truth can be a real-world experiment, a simulation, or something else. Regardless, it must be \"where the buck stops\". The source of truth is considered the \"right answer\" with respect to the estimator prediction.\n",
    "\n",
    "In this notebook, we create some dummy training data and also get a source of truth (a function called `truth`) which can be queried to get the \"right\" answer. However, in principle, you can define the `truth` function to be whatever you want. It must only take an input in the same vector space as your estimator's inputs, and return an output in the same vector space as your estimator's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59dccd-366e-4cf1-97ce-976d353e00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x1, grid_x2, train_x, train_y, truth, truth_meshgrid = get_dummy_2d_data(seed=124)\n",
    "grid = grids_to_coordinates([grid_x1, grid_x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da6c6a-2269-404c-8b96-d6a07fb12b2d",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1faaa-6913-4a5a-adfc-f8d0b21eded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension = 2\n",
    "X = np.array([0.234, 0.567]).reshape(1, input_dimension)\n",
    "truth(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6238443-9380-433a-b588-4a1fc27e1915",
   "metadata": {},
   "source": [
    "## The estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e1543-adf8-43f0-b917-1799606eae08",
   "metadata": {},
   "source": [
    "In this work, we will be using a **Gaussian Process** (GP) as our estimator for modeling the space. Specifically, suppose the input dimension is $D$, and the output dimension is 1. The GP is a mapping from $\\mathbb{R}^D \\mapsto \\mathbb{R}$. The GP can also be sampled to produce a measure of uncertainty given some input point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d84ef-3b82-4fec-82bc-bcf0026efc83",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d3854-7de8-4ddf-8e99-1ef05b1c151b",
   "metadata": {},
   "source": [
    "Suppose we have an experiment where observations are acquired sequentially, but on an arbitrary delay. In other words, suppose we propose an experiment on iteration `i`. That experiment's results will not be observed until the (`i+n`)th iteration. `n=1` corresponds to a delay time of zero.\n",
    "\n",
    "To start the experiment, we choose some random points within the bounds of the experiment we wish to perform. In this simple example, we have a dimension `D=2` and, let's say, we have a delay time of `n=5`. Thus, we should propose 5 experiments to start. Once the first of these is observed, there will be 4 remaining in the queue, and our next proposed experiment should take into account that we have 4 points still in the queue.\n",
    "\n",
    "In order to bootstrap the experiment, let's choose 4 random points from a `LatinHypercube` sampler in the space `D=2` subject to the constraints $x_1 \\in [-4, 5]$ and $x_2 \\in [-5, 4]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cc93d-bce0-4506-bade-82ee8bb542e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [[-4.0, 5.0], [-5.0, 4.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c6802-2bfa-423e-aa5b-8011327d911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pending = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51446ae-777b-4fdd-af4f-0991fc284a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.qmc import LatinHypercube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef151e-a835-46f1-b5cd-2a6149ce7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = LatinHypercube(d=2, seed=123)\n",
    "sample = sampler.random(n=n_pending)\n",
    "\n",
    "# We should scale the sample to our boundaries\n",
    "sample[:, 0] = (bounds[0][1] - bounds[0][0]) * sample[:, 0] + bounds[0][0]\n",
    "sample[:, 1] = (bounds[1][1] - bounds[1][0]) * sample[:, 1] + bounds[1][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2685d87-16db-4168-a4a3-bea71a6622c3",
   "metadata": {},
   "source": [
    "## Running the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ca8d6-be17-4d53-b60f-2d819f94a47d",
   "metadata": {},
   "source": [
    "We now have our initial data, and our source of truth. All that remains is to run the experiment! The general process will be as follows:\n",
    "\n",
    "1. Pop the queue to get the experiment that will be observed next.\n",
    "2. Observe the result of that experiment.\n",
    "3. Add that input-output pair to the training set.\n",
    "4. Condition and train a GP on the current data.\n",
    "5. Use that GP, acquisition function and optimizer to find the next experiment, _given_ the current pending experiments.\n",
    "6. Add that new point to the queue.\n",
    "7. Repeat from step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8378e-6ad0-46d2-b757-6835796641eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = train_x.copy()\n",
    "# Y = train_y.copy()\n",
    "\n",
    "X = sample.copy()\n",
    "Y = truth(X).reshape(-1, 1)\n",
    "pointer = 1\n",
    "normalize_inputs_to_unity = True\n",
    "\n",
    "with logging_mode(success=True, warning=False):\n",
    "    for iteration in tqdm(range(85), disable=True):\n",
    "\n",
    "            # Condition the GP on the current data\n",
    "            model = EasySingleTaskGPRegressor(\n",
    "                train_x=X[:pointer + 1, :].copy(),\n",
    "                train_y=Y[:pointer + 1, :].copy(),\n",
    "                normalize_inputs_to_unity=normalize_inputs_to_unity,\n",
    "                standardize_outputs=True\n",
    "            )\n",
    "\n",
    "            # Train the hyperparameters. In the case where this fails (it sometimes does,\n",
    "            # see here, for example: `https://stats.stackexchange.com/questions/547490/\n",
    "            # gaussian-process-regression-normalization-of-data-worsens-fit-why`),\n",
    "            # we adjust the way the model scales its input parameterrs\n",
    "            model.train_()\n",
    "            if not model.training_state_successful:\n",
    "                normalize_inputs_to_unity = False\n",
    "                continue\n",
    "\n",
    "            # Ask the BO engine which next point we should use\n",
    "            new_points = ask(\n",
    "                model=model,\n",
    "                bounds=bounds,\n",
    "                X_pending=X[pointer + 1:, :].copy(),\n",
    "                acquisition_function=\"qMaxVariance\",\n",
    "                acquisition_function_kwargs=dict(),\n",
    "                optimize_acqf_kwargs={\"q\": 1, \"num_restarts\": 5, \"raw_samples\": 20}\n",
    "            )\n",
    "            \n",
    "            # Get the current observation. Here, `truth` will have to be implemented in a real experiment!\n",
    "            current_obs = truth(new_points).reshape(-1, 1)\n",
    "\n",
    "            # Add this new point to the queue\n",
    "            X = np.concatenate([X, new_points.reshape(1, input_dimension)], axis=0)\n",
    "            Y = np.concatenate([Y, current_obs], axis=0)\n",
    "\n",
    "            # Repeat!\n",
    "            pointer += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159b6a8-478a-4de1-860f-420816314f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(grid=grid)\n",
    "mu = pred[\"mean\"].reshape(len(grid_x2), len(grid_x1))\n",
    "z = truth_meshgrid(grid_x1, grid_x2)\n",
    "z_min = -np.abs(z).max()\n",
    "z_max = np.abs(z).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894f13f-24ee-491b-89da-3b40a5804d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(4, 2), sharey=True, sharex=True)\n",
    "\n",
    "ax = axs[0]\n",
    "c = ax.imshow(\n",
    "    z.T, cmap='rainbow', vmin=z_min, vmax=z_max,\n",
    "    extent=[grid_x1.min(), grid_x1.max(), grid_x2.min(), grid_x2.max()],\n",
    "    interpolation ='nearest', origin ='lower'\n",
    ")\n",
    "set_grids(ax, grid=False)\n",
    "ax.set_title(\"Function\")\n",
    "\n",
    "ax = axs[1]\n",
    "c = ax.imshow(\n",
    "    mu, cmap='rainbow', vmin=z_min, vmax=z_max,\n",
    "    extent=[grid_x1.min(), grid_x1.max(), grid_x2.min(), grid_x2.max()],\n",
    "    interpolation ='nearest', origin ='lower'\n",
    ")\n",
    "set_grids(ax, grid=False)\n",
    "ax.scatter(X[:, 0], X[:, 1], s=5, color=\"black\")\n",
    "ax.scatter(sample[:, 0], sample[:, 1], s=3, color=\"blue\")\n",
    "ax.set_title(\"GP\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
